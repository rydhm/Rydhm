{"cells":[{"cell_type":"markdown","metadata":{"_uuid":"bda30bb60722647ec9219ff4644dbf09991582bb"},"source":["* **This kernel is based on one of the exercises in the excellent book: [Deep Learning with Python by Francois Chollet](https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438)\n","**\n","* The kernel imports the IMDB reviews (originally text - already transformed by Keras to integers using a dictionary)\n","* Vectorizes and normalizes the data\n","* Compiles a multi layers NN\n","* Monitors the learning / validation curves for loss and accuracy\n","* Try and error with different layers and hidden units\n","* Employs L1 and L2 weight regularization\n","* Implements a DROPOUT layer\n","\n","* The above mentioned book is a **MUST READ**.\n","* *Thanks Francois for an amazing book !*"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# IMPORT MODULES\n","# TURN ON the GPU !!!\n","# If importing dataset from outside - like this IMDB - Internet must be \"connected\"\n","\n","import os\n","from operator import itemgetter    \n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import warnings\n","warnings.filterwarnings('ignore')\n","get_ipython().magic(u'matplotlib inline')\n","plt.style.use('ggplot')\n","\n","import tensorflow as tf\n","\n","from keras import models, regularizers, layers, optimizers, losses, metrics\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.utils import np_utils, to_categorical\n"," \n","from keras.datasets import imdb\n","\n","print(os.getcwd())\n","print(\"Modules imported \\n\")\n","print(\"Files in current directory:\")\n","from subprocess import check_output\n","print(check_output([\"ls\", \"../input\"]).decode(\"utf8\")) #check the files available in the directory"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"2309a027762ad0f83018d32eb194af33bf128021","collapsed":true,"trusted":true},"outputs":[],"source":["# LOAD IMDB DATA\n","\n","(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n","num_words=10000)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"563a5a8c65a26338fc42b0bcad94e3e4782be0b6","trusted":true},"outputs":[],"source":["print(\"train_data \", train_data.shape)\n","print(\"train_labels \", train_labels.shape)\n","print(\"_\"*100)\n","print(\"test_data \", test_data.shape)\n","print(\"test_labels \", test_labels.shape)\n","print(\"_\"*100)\n","print(\"Maximum value of a word index \")\n","print(max([max(sequence) for sequence in train_data]))\n","print(\"Maximum length num words of review in train \")\n","print(max([len(sequence) for sequence in train_data]))"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"5db6853639628b911ccdd0e0ecc484f4beeb9284","trusted":true},"outputs":[],"source":["# See an actual review in words\n","# Reverse from integers to words using the DICTIONARY (given by keras...need to do nothing to create it)\n","\n","word_index = imdb.get_word_index()\n","\n","reverse_word_index = dict(\n","[(value, key) for (key, value) in word_index.items()])\n","\n","decoded_review = ' '.join(\n","[reverse_word_index.get(i - 3, '?') for i in train_data[123]])\n","\n","print(decoded_review)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"080f1f790e7294231c1e62c1688666589d51a200","collapsed":true,"trusted":true},"outputs":[],"source":["# VECTORIZE as one cannot feed integers into a NN \n","# Encoding the integer sequences into a binary matrix - one hot encoder basically\n","# From integers representing words, at various lengths - to a normalized one hot encoded tensor (matrix) of 10k columns\n","\n","def vectorize_sequences(sequences, dimension=10000):\n","    results = np.zeros((len(sequences), dimension))\n","    for i, sequence in enumerate(sequences):\n","        results[i, sequence] = 1.\n","    return results"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"f012f8661fd781a95bd6c01197b7a93e944ff38e","trusted":true},"outputs":[],"source":["x_train = vectorize_sequences(train_data)\n","x_test = vectorize_sequences(test_data)\n","\n","print(\"x_train \", x_train.shape)\n","print(\"x_test \", x_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"d561665727e49217e14c86804f148adb6899a7b7","trusted":true},"outputs":[],"source":["# VECTORIZE the labels too - NO INTEGERS only floats into a tensor...(rare exceptions)\n","\n","y_train = np.asarray(train_labels).astype('float32')\n","y_test = np.asarray(test_labels).astype('float32')\n","print(\"y_train \", y_train.shape)\n","print(\"y_test \", y_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"bccb4315c1d69c18cab22a633dc68abdff4b35fb","trusted":true},"outputs":[],"source":["# Set a VALIDATION set\n","\n","x_val = x_train[:10000]\n","partial_x_train = x_train[10000:]\n","y_val = y_train[:10000]\n","partial_y_train = y_train[10000:]\n","\n","print(\"x_val \", x_val.shape)\n","print(\"partial_x_train \", partial_x_train.shape)\n","print(\"y_val \", y_val.shape)\n","print(\"partial_y_train \", partial_y_train.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"cf2a055346b2e3252b103f1d0bab9e786e5d5eba","trusted":true},"outputs":[],"source":["# NN MODEL\n","\n","# Use of DROPOUT\n","model = models.Sequential()\n","model.add(layers.Dense(16, kernel_regularizer=regularizers.l1(0.001), activation='relu', input_shape=(10000,)))\n","model.add(layers.Dropout(0.5))\n","model.add(layers.Dense(16, kernel_regularizer=regularizers.l1(0.001),activation='relu'))\n","model.add(layers.Dropout(0.5))\n","model.add(layers.Dense(1, activation='sigmoid'))\n","\n","# Use of REGULARIZATION\n","#model = models.Sequential()\n","#model.add(layers.Dense(16, kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001),activation='relu', input_shape=(10000,)))\n","#model.add(layers.Dense(16, kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001),activation='relu'))\n","#model.add(layers.Dense(1, activation='sigmoid'))\n","\n","# REGULARIZERS L1 L2\n","#regularizers.l1(0.001)\n","#regularizers.l2(0.001)\n","#regularizers.l1_l2(l1=0.001, l2=0.001)\n","\n","# OPTIMIZERS\n","#model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss=losses.binary_crossentropy, metrics=[metrics.binary_accuracy])\n","#model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"356f3351bfa767aca48e91d305961dda2d3b9a9d","scrolled":true,"trusted":true},"outputs":[],"source":["# FIT / TRAIN model\n","\n","NumEpochs = 10\n","BatchSize = 512\n","\n","model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n","\n","history = model.fit(partial_x_train, partial_y_train, epochs=NumEpochs, batch_size=BatchSize, validation_data=(x_val, y_val))\n","\n","results = model.evaluate(x_test, y_test)\n","print(\"_\"*100)\n","print(\"Test Loss and Accuracy\")\n","print(\"results \", results)\n","history_dict = history.history\n","history_dict.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"41489e2204c5d4159f5c90a350aa3ad839e03355","trusted":true},"outputs":[],"source":["# VALIDATION LOSS curves\n","\n","plt.clf()\n","history_dict = history.history\n","loss_values = history_dict['loss']\n","val_loss_values = history_dict['val_loss']\n","epochs = range(1, (len(history_dict['loss']) + 1))\n","plt.plot(epochs, loss_values, 'bo', label='Training loss')\n","plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"abfa37123f9f403041a1c12edd6a87070b78c5ef","trusted":true},"outputs":[],"source":["# VALIDATION ACCURACY curves\n","\n","plt.clf()\n","acc_values = history_dict['acc']\n","val_acc_values = history_dict['val_acc']\n","epochs = range(1, (len(history_dict['acc']) + 1))\n","plt.plot(epochs, acc_values, 'bo', label='Training acc')\n","plt.plot(epochs, val_acc_values, 'b', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"2fda90fab3c7038df8291fa4f3185fbb62e9aed4","trusted":true},"outputs":[],"source":["# PREDICT\n","\n","model.predict(x_test)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}
